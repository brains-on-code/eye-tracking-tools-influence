{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8d805e",
   "metadata": {},
   "source": [
    "Data Analysis with PyGazeAnalyzer 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6504e3b5-6716-4b57-8bcb-fc96da173344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def remove_missing(x, y, time, missing):\n",
    "\tmx = numpy.array(x==missing, dtype=int)\n",
    "\tmy = numpy.array(y==missing, dtype=int)\n",
    "\tx = x[(mx+my) != 2]\n",
    "\ty = y[(mx+my) != 2]\n",
    "\ttime = time[(mx+my) != 2]\n",
    "\treturn x.reset_index(drop=True), y.reset_index(drop=True), time.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def fixation_detection_fixed(x, y, time, missing=0.0, maxdist=25, mindur=50):\n",
    "\t\n",
    "\t\"\"\"Detects fixations, defined as consecutive samples with an inter-sample\n",
    "\tdistance of less than a set amount of pixels (disregarding missing data)\n",
    "\t\n",
    "\targuments\n",
    "\n",
    "\tx\t\t-\tnumpy array of x positions\n",
    "\ty\t\t-\tnumpy array of y positions\n",
    "\ttime\t\t-\tnumpy array of EyeTribe timestamps\n",
    "\n",
    "\tkeyword arguments\n",
    "\n",
    "\tmissing\t-\tvalue to be used for missing data (default = 0.0)\n",
    "\tmaxdist\t-\tmaximal inter sample distance in pixels (default = 25)\n",
    "\tmindur\t-\tminimal duration of a fixation in milliseconds; detected\n",
    "\t\t\t\tfixation cadidates will be disregarded if they are below\n",
    "\t\t\t\tthis duration (default = 100)\n",
    "\t\n",
    "\treturns\n",
    "\tSfix, Efix\n",
    "\t\t\t\tSfix\t-\tlist of lists, each containing [starttime]\n",
    "\t\t\t\tEfix\t-\tlist of lists, each containing [starttime, endtime, duration, endx, endy]\n",
    "\t\"\"\"\n",
    "\t#print(\"fixation_detection_fixed\", x, y, time, missing, maxdist, mindur)\n",
    "    \n",
    "\tx, y, time = remove_missing(x, y, time, missing)\n",
    "    \n",
    "\t#print(\"remove_missing\",x,y, time)\n",
    "\n",
    "\t# empty list to contain data\n",
    "\tSfix = []\n",
    "\tEfix = []\n",
    "\t\n",
    "\t# loop through all coordinates\n",
    "\tsi = 0\n",
    "\tfixstart = False\n",
    "\tfor i in range(1,len(x)):\n",
    "\t\t# calculate Euclidean distance from the current fixation coordinate\n",
    "\t\t# to the next coordinate\n",
    "\t\tsquared_distance = ((x[si]-x[i])**2 + (y[si]-y[i])**2)\n",
    "\t\tdist = 0.0\n",
    "\n",
    "\t\tif squared_distance > 0:\n",
    "\t\t\tdist = squared_distance**0.5\n",
    "            \n",
    "\t\t#print(\"loop\", i, squared_distance, dist)\n",
    "        \n",
    "\t\t# check if the next coordinate is below maximal distance\n",
    "\t\tif dist <= maxdist and not fixstart:\n",
    "\t\t\t# start a new fixation\n",
    "\t\t\tsi = 0 + i\n",
    "\t\t\tfixstart = True\n",
    "\t\t\tSfix.append([time[i]])\n",
    "\t\telif dist > maxdist and fixstart:\n",
    "\t\t\t# end the current fixation\n",
    "\t\t\tfixstart = False\n",
    "\t\t\t# only store the fixation if the duration is ok\n",
    "\t\t\tif time[i-1]-Sfix[-1][0] >= mindur:\n",
    "\t\t\t\tEfix.append([Sfix[-1][0], time[i-1], time[i-1]-Sfix[-1][0], x[si], y[si]])\n",
    "\t\t\t# delete the last fixation start if it was too short\n",
    "\t\t\telse:\n",
    "\t\t\t\tSfix.pop(-1)\n",
    "\t\t\tsi = 0 + i\n",
    "\t\telif not fixstart:\n",
    "\t\t\tsi += 1\n",
    "\t#add last fixation end (we can lose it if dist > maxdist is false for the last point)\n",
    "\tif len(Sfix) > len(Efix):\n",
    "\t\tEfix.append([Sfix[-1][0], time[len(x)-1], time[len(x)-1]-Sfix[-1][0], x[si], y[si]])\n",
    "\treturn Sfix, Efix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db788c8-02d4-4694-bd98-05d218458710",
   "metadata": {},
   "source": [
    "Saccade Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068f0a42-be46-4dbf-8bbe-233962dc6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saccade_detection_fixed(x, y, time, missing=0.0, minlen=5, maxvel=40, maxacc=340):\n",
    "\t\n",
    "\t\"\"\"Detects saccades, defined as consecutive samples with an inter-sample\n",
    "\tvelocity of over a velocity threshold or an acceleration threshold\n",
    "\t\n",
    "\targuments\n",
    "\n",
    "\tx\t\t-\tnumpy array of x positions\n",
    "\ty\t\t-\tnumpy array of y positions\n",
    "\ttime\t\t-\tnumpy array of tracker timestamps in milliseconds\n",
    "\n",
    "\tkeyword arguments\n",
    "\n",
    "\tmissing\t-\tvalue to be used for missing data (default = 0.0)\n",
    "\tminlen\t-\tminimal length of saccades in milliseconds; all detected\n",
    "\t\t\t\tsaccades with len(sac) < minlen will be ignored\n",
    "\t\t\t\t(default = 5)\n",
    "\tmaxvel\t-\tvelocity threshold in pixels/second (default = 40)\n",
    "\tmaxacc\t-\tacceleration threshold in pixels / second**2\n",
    "\t\t\t\t(default = 340)\n",
    "\t\n",
    "\treturns\n",
    "\tSsac, Esac\n",
    "\t\t\tSsac\t-\tlist of lists, each containing [starttime]\n",
    "\t\t\tEsac\t-\tlist of lists, each containing [starttime, endtime, duration, startx, starty, endx, endy]\n",
    "\t\"\"\"\n",
    "\tx, y, time = remove_missing(x, y, time, missing)\n",
    "\n",
    "\t# CONTAINERS\n",
    "\tSsac = []\n",
    "\tEsac = []\n",
    "\n",
    "\t# INTER-SAMPLE MEASURES\n",
    "\t# the distance between samples is the square root of the sum\n",
    "\t# of the squared horizontal and vertical interdistances\n",
    "\tintdist = (numpy.diff(x)**2 + numpy.diff(y)**2)**0.5\n",
    "\t# get inter-sample times\n",
    "\tinttime = numpy.diff(time)\n",
    "\n",
    "\t# recalculate inter-sample times to seconds\n",
    "\tinttime = inttime / 1000.0\n",
    "\t\n",
    "\t# VELOCITY AND ACCELERATION\n",
    "\t# the velocity between samples is the inter-sample distance\n",
    "\t# divided by the inter-sample time\n",
    "\tvel = intdist / inttime\n",
    "\t# the acceleration is the sample-to-sample difference in\n",
    "\t# eye movement velocity\n",
    "\tacc = numpy.diff(vel)\n",
    "\n",
    "\t# SACCADE START AND END\n",
    "\tt0i = 0\n",
    "\tstop = False\n",
    "\twhile not stop:\n",
    "\t\t# saccade start (t1) is when the velocity or acceleration\n",
    "\t\t# surpass threshold, saccade end (t2) is when both return\n",
    "\t\t# under threshold\n",
    "\t\n",
    "\t\t# detect saccade starts\n",
    "\t\tsacstarts = numpy.where((vel[1+t0i:] > maxvel).astype(int) + (acc[t0i:] > maxacc).astype(int) >= 1)[0]\n",
    "\t\tif len(sacstarts) > 0:\n",
    "\t\t\t# timestamp for starting position\n",
    "\t\t\tt1i = t0i + sacstarts[0] + 1\n",
    "\t\t\tif t1i >= len(time)-1:\n",
    "\t\t\t\tt1i = len(time)-2\n",
    "\t\t\tt1 = time[t1i]\n",
    "\t\t\t\n",
    "\t\t\t# add to saccade starts\n",
    "\t\t\tSsac.append([t1])\n",
    "\t\t\t\n",
    "\t\t\t# detect saccade endings\n",
    "\t\t\tsacends = numpy.where((vel[1+t1i:] < maxvel).astype(int) + (acc[t1i:] < maxacc).astype(int) == 2)[0]\n",
    "\t\t\tif len(sacends) > 0:\n",
    "\t\t\t\t# timestamp for ending position\n",
    "\t\t\t\tt2i = sacends[0] + 1 + t1i + 2\n",
    "\t\t\t\tif t2i >= len(time):\n",
    "\t\t\t\t\tt2i = len(time)-1\n",
    "\t\t\t\tt2 = time[t2i]\n",
    "\t\t\t\tdur = t2 - t1\n",
    "\n",
    "\t\t\t\t# ignore saccades that did not last long enough\n",
    "\t\t\t\tif dur >= minlen:\n",
    "\t\t\t\t\t# add to saccade ends\n",
    "\t\t\t\t\tEsac.append([t1, t2, dur, x[t1i], y[t1i], x[t2i], y[t2i]])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# remove last saccade start on too low duration\n",
    "\t\t\t\t\tSsac.pop(-1)\n",
    "\n",
    "\t\t\t\t# update t0i\n",
    "\t\t\t\tt0i = 0 + t2i\n",
    "\t\t\telse:\n",
    "\t\t\t\tstop = True\n",
    "\t\telse:\n",
    "\t\t\tstop = True\n",
    "\t\n",
    "\treturn Ssac, Esac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee044b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def call_fixation_detection_on_data(fixation_info, participant, time, x, y, task = 0, missing=0.0, maxdist=25, mindur=50):\n",
    "\tmissing = missing  # Specify the missing value threshold (if any)\n",
    "\tmaxdist = maxdist  # Maximum distance for a fixation (adjust as needed)\n",
    "\tmindur = mindur  # Minimum duration for a fixation (adjust as needed)\n",
    "\n",
    "\t# Perform fixation detection using the fixed fixation_detection function\n",
    "\tSfix, Efix = fixation_detection_fixed(x, y, time, missing=missing, maxdist=maxdist, mindur=mindur)\n",
    "\n",
    "\t# Calculate total fixation duration and average fixation duration\n",
    "\ttotal_duration = sum(sublist[2] for sublist in Efix)\n",
    "\taverage_duration = total_duration / len(Efix) if len(Efix) > 0 else 0\n",
    "\n",
    "\t# Update the fixation_info dictionary\n",
    "\tfixation_info['Participant'].append(participant)\n",
    "\tfixation_info['Task'].append(task)\n",
    "\tfixation_info['Fixation Count'].append(len(Efix))\n",
    "\tfixation_info['Total Fixation Duration [ms]'].append(total_duration)\n",
    "\tfixation_info['Average Fixation Duration [ms]'].append(average_duration)\n",
    "\n",
    "def call_saccade_detection_on_data(saccade_info, participant, time, x, y, task = 0):\n",
    "    missing = 0.0  # Specify the missing value threshold (if any)\n",
    "    minlen = 5  # Maximum distance for a saccade (adjust as needed)\n",
    "    maxvel = 40  # Minimum duration for a saccade (adjust as needed)\n",
    "    maxacc = 340\n",
    "\n",
    "    # Perform saccade detection using the saccade_detection function\n",
    "    Ssac, Esac = saccade_detection_fixed(x, y, time, missing=missing, minlen=minlen, maxvel=maxvel, maxacc=maxacc)\n",
    "\n",
    "    # Calculate total saccade duration and average saccade duration\n",
    "    total_duration = sum(sublist[2] for sublist in Esac)\n",
    "    average_duration = total_duration / len(Esac) if len(Esac) > 0 else 0\n",
    "\n",
    "    # Update the saccade_info dictionary\n",
    "    saccade_info['Participant'].append(participant)\n",
    "    saccade_info['Task'].append(task)\n",
    "    saccade_info['Saccade Count'].append(len(Esac))\n",
    "    saccade_info['Total Saccade Duration [ms]'].append(total_duration)\n",
    "    saccade_info['Average Saccade Duration [ms]'].append(average_duration)\n",
    "\n",
    "\n",
    "'''\n",
    "def prepare_tobii_data(directory_path, file_name, fixation_info, fn):\n",
    "\ttsv_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\t# Load the Tobii eye tracker data into a Pandas DataFrame\n",
    "\tdf = pd.read_csv(tsv_file, delimiter='\\t', low_memory=False)\n",
    "\tdf = df[['Gaze point X [DACS px]', 'Gaze point Y [DACS px]', 'Recording timestamp [ms]']]\n",
    "\tdf = df.fillna(0.0)\n",
    "\n",
    "\t# Define parameters for fixation detection\n",
    "\tx = df['Gaze point X [DACS px]']  # X-coordinate data\n",
    "\ty = df['Gaze point Y [DACS px]']  # Y-coordinate data\n",
    "\ttime = df['Recording timestamp [ms]']\n",
    "\n",
    "\tfn(fixation_info, os.path.splitext(file_name)[0], time, x, y)\n",
    "      \n",
    "'''\n",
    "\n",
    "def prepare_tobii_data(directory_path, file_name, fixation_info, fn):\n",
    "\ttsv_file = os.path.join(directory_path, file_name)\n",
    "\tprint(\"prepare_tobii_data\", tsv_file)\n",
    "\n",
    "\t# Load the Tobii eye tracker data into a Pandas DataFrame and skip lines that start with ## as they are comments\n",
    "\n",
    "\tdf = None\n",
    "\tpossible_skipped_rows = [37, 32, 41, 45]\n",
    "\tcounter = 0\n",
    "\twhile df is None:\n",
    "\t\ttry:\n",
    "\t\t\tdf = pd.read_csv(tsv_file, delimiter='\\t', low_memory=False, on_bad_lines='skip', skiprows=possible_skipped_rows[counter])\n",
    "\t\t\tdf = df[['Time', 'Type', 'L Raw X [px]', 'L Raw Y [px]', 'R Raw X [px]', 'R Raw Y [px]', 'L Validity', 'R Validity', 'R POR X [px]', 'R POR Y [px]']]\n",
    "\t\texcept KeyError as e:\n",
    "\t\t\tcounter = counter + 1\n",
    "\t\t\tdf = None\n",
    "\t\t\tif counter >= len(possible_skipped_rows):\n",
    "\t\t\t\tprint(\"no possible skipped rows worked\", e)\n",
    "\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tprint(\"possible skipped rows worked\", possible_skipped_rows[counter])\n",
    "\t\t\tbreak\n",
    "\n",
    "\tdf['Type'] = df['Type'].astype(str)\n",
    "\tdf = df.fillna(0.0)\n",
    "\n",
    "\t# Get the row numbers where Type is 'MSG'\n",
    "\tmsg_rows = df[df['Type'] == 'MSG'].index\n",
    "\n",
    "\t# For each msg_row, split the df into two dataframes, one before the msg_row and one after\n",
    "\t# Run the analysis for each of those split dataframes as they represent different tasks\n",
    "\tdf_after_msg = df\n",
    "\tlast_task_name = None\n",
    "\tlast_task_row_number = 0\n",
    "\tremoved_task_names = {\n",
    "\t\t'instruction_calibration.jpg': True,\n",
    "\t\t'instruction_comprehension.jpg': True,\n",
    "\t}\n",
    "\n",
    "\tfor msg_row in msg_rows:\n",
    "\t\tcurrent_task_name = df['L Raw X [px]'][msg_row].split('Message: ')[1]\n",
    "\t\tif '.jpg' in current_task_name:\t\t\t\n",
    "\t\t\tif last_task_name is not None and last_task_name not in removed_task_names:\n",
    "\t\t\t\t\n",
    "\t\t\t\tdf_msg = df[last_task_row_number:msg_row]\n",
    "\t\t\t\t# Remove all rows at least one eye is invalid\n",
    "\t\t\t\t#df_msg = df_msg[(df_msg['L Validity'] == 1) & (df_msg['R Validity'] == 1)]\n",
    "\t\t\t\tdf_msg = df_msg[(df_msg['R Validity'] == 1)]\n",
    "\t\t\t\t'''\n",
    "\t\t\t\t# Define parameters for fixation detection\n",
    "\t\t\t\tx_left = df_msg['L Raw X [px]'].astype(float)\n",
    "\t\t\t\tx_right = df_msg['R Raw X [px]'] \n",
    "\t\t\t\tx = (x_left + x_right) / 2\n",
    "\n",
    "\t\t\t\ty_left = df_msg['L Raw Y [px]']\n",
    "\t\t\t\ty_right = df_msg['R Raw Y [px]']\n",
    "\t\t\t\ty = (y_left + y_right) / 2\n",
    "\t\t\t\t'''\n",
    "\n",
    "\t\t\t\t# Define parameters for fixation detection\n",
    "\t\t\t\tx_left = df_msg.loc[:,('L Raw X [px]')].astype(float)\n",
    "\t\t\t\t#x_right = df_msg.loc[:,('R Raw X [px]')]\n",
    "\t\t\t\tx_right = df_msg.loc[:,('R POR X [px]')]\n",
    "\t\t\t\t#df_msg.loc[:,('X')] = (x_left + x_right) / 2\n",
    "\t\t\t\tdf_msg.loc[:,('X')] = x_right\n",
    "\n",
    "\t\t\t\ty_left = df_msg['L Raw Y [px]']\n",
    "\t\t\t\t#y_right = df_msg['R Raw Y [px]']\n",
    "\t\t\t\ty_right = df_msg.loc[:,('R POR Y [px]')]\n",
    "\t\t\t\t#df_msg.loc[:,('Y')] = (y_left + y_right) / 2\n",
    "\t\t\t\tdf_msg.loc[:,('Y')] = y_right\n",
    "\n",
    "\t\t\t\t# Normalize time\n",
    "\t\t\t\ttime = df_msg['Time'] - df_msg['Time'].min()\n",
    "\t\t\t\t# Time conversion from microseconds to milliseconds\n",
    "\t\t\t\ttime = time / 1000\n",
    "\n",
    "\t\t\t\tparticipant_id = file_name.split('_')[0]\n",
    "\t\t\t\t#fn(fixation_info, participant_id, time, x, y, last_task_name)\n",
    "\t\t\t\tfn(fixation_info, participant_id, time, x_right, y_right, last_task_name)\n",
    "\n",
    "\t\t\tlast_task_name = current_task_name\n",
    "\t\t\tlast_task_row_number = msg_row\n",
    "\n",
    "\t\t\n",
    "\tdf_msg = df[last_task_row_number:]\n",
    "\t# Remove all rows at least one eye is invalid\n",
    "\t#df_msg = df_msg[(df_msg['L Validity'] == 1) & (df_msg['R Validity'] == 1)]\n",
    "\tdf_msg = df_msg[(df_msg['R Validity'] == 1)]\n",
    "\n",
    "\t# Define parameters for fixation detection\n",
    "\tx_left = df_msg['L Raw X [px]'].astype(float)\n",
    "\tx_right = df_msg['R POR X [px]'] \n",
    "\tx = (x_left + x_right) / 2\n",
    "\n",
    "\ty_left = df_msg['L Raw Y [px]']\n",
    "\ty_right = df_msg['R POR Y [px]']\n",
    "\ty = (y_left + y_right) / 2\n",
    "\t# Normalize time\n",
    "\ttime = df_msg['Time'] - df_msg['Time'].min()\n",
    "\t# Time conversion from microseconds to milliseconds\n",
    "\ttime = time / 1000\n",
    "\n",
    "\tparticipant_id = file_name.split('_')[0]\n",
    "\tfn(fixation_info, participant_id, time, x_right, y_right, last_task_name)\n",
    "\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "def prepare_csv_data(directory_path, file_name, fixation_info, fn):\n",
    "\tcsv_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\tfile_name_split = file_name.split('-')\n",
    "\ttask_number = file_name_split[0][1:]\n",
    "\tparticipant_id = file_name_split[1]\n",
    "\n",
    "\t# Load the eye tracker data into a Pandas DataFrame\n",
    "\tdf = pd.read_csv(csv_file, delimiter=',', low_memory=False, on_bad_lines='skip', dtype = {'x': float, 'y': str, 'time': float})\n",
    "\ttime_column = df.filter(like='TIME(')\n",
    "\tdf = df[['FPOGX', 'FPOGY']]\n",
    "\n",
    "\ttime_column = time_column.rename(columns={time_column.columns[0]: 'TIME'})\n",
    "\n",
    "\t# Add the renamed column to the original DataFrame\n",
    "\tdf = pd.concat([df, time_column], axis=1)\n",
    "\n",
    "\tdf = df.fillna(0.0)\n",
    "\tdf['FPOGX'] = df['FPOGX'] * 1920\n",
    "\tdf['FPOGY'] = df['FPOGY'] * 1080\n",
    "\n",
    "\n",
    "\t# convert time to milliseconds\n",
    "\tdf['TIME'] = df['TIME']*1000\n",
    "\n",
    "\n",
    "\t# Call the defined fn\n",
    "\tfn(fixation_info, participant_id, df['TIME'], df['FPOGX'], df['FPOGY'], task_number)\n",
    "\n",
    "\n",
    "def prepare_xml_data(directory_path, file_name, fixation_info, fn):\n",
    "\t# We only want the eclipse xml files as those have the x and y coordinates\n",
    "\tif not 'eclipse' in file_name:\n",
    "\t\treturn\n",
    "\n",
    "\txml_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\tpath_elements = directory_path.split(os.sep)\n",
    "\tparticipant_id = path_elements[-3]\n",
    "\ttask = path_elements[-2].split('-')[2]\n",
    "\n",
    "\ttree = ET.parse(xml_file)\n",
    "\troot = tree.getroot()\n",
    "\n",
    "\t# Extract data from XML\n",
    "\tdata = []\n",
    "\tfor response_elem in root.findall(\".//response\"):\n",
    "\t\tx = float(response_elem.get(\"x\"))\n",
    "\t\ty = float(response_elem.get(\"y\"))\n",
    "\t\t# Event time is in nanoseconds, so we divide by 1000000 to get milliseconds\n",
    "\t\tevent_time = int(response_elem.get(\"event_time\")) / 1000000\n",
    "\t\t\n",
    "\t\tdata.append({\"event_time\": event_time, \"x\": x, \"y\": y})\n",
    "\n",
    "\t# Create DataFrame\n",
    "\tdf = pd.DataFrame(data)\n",
    "\n",
    "\tfn(fixation_info, participant_id, df['event_time'], df['x'], df['y'], task)\n",
    "\n",
    "\n",
    "def prepare_txt_data(directory_path, file_name, fixation_info, fn):\n",
    "\t# We only want the ogama txt files as those have the x and y coordinates\n",
    "\tif not 'ogama' in file_name:\n",
    "\t\treturn\n",
    "\ttxt_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\tpath_elements = directory_path.split(os.sep)\n",
    "\tparticipant_id = path_elements[-1]\n",
    "\n",
    "\t# Load the eye tracker data into a Pandas DataFrame\n",
    "\tdf = pd.read_csv(txt_file, delimiter=',', low_memory=False, on_bad_lines='skip', encoding = \"utf-16\")\n",
    "\tdf = df[[' ImageName', ' X', ' Y', ' StartTime', ' Included?', ' StimulusType']]\n",
    "\n",
    "\tdf[' X'] = df[' X'] * 1920 / 1024\n",
    "\tdf[' Y'] = df[' Y'] * 1080 / 768\n",
    "\n",
    "\t# Remove all rows where Included? == N\n",
    "\tdf = df[df[' Included?'] == 'Y']\n",
    "\n",
    "\tdf = df.fillna(0.0)\n",
    "\n",
    "\t\n",
    "\t# Iterate over all unique images\n",
    "\tfor image_name in df[' ImageName'].unique():\n",
    "\t\t# Get the data for the current image\n",
    "\t\timage_df = df[df[' ImageName'] == image_name]\n",
    "\n",
    "\t\t# Call the defined fn\n",
    "\t\tfn(fixation_info, participant_id, image_df[' StartTime'], image_df[' X'], image_df[' Y'], image_name, None, 50)\n",
    "\t\n",
    "\n",
    "ending_to_function = {\n",
    "    '.tsv': prepare_tobii_data,\n",
    "    '.csv': prepare_csv_data,\n",
    "\t'.xml': prepare_xml_data,\n",
    "\t'.txt': prepare_txt_data\n",
    "}\n",
    "\n",
    "test_fixation_info = {\n",
    "        'Participant': [],\n",
    "        'Task': [],\n",
    "        'Fixation Count': [],\n",
    "        'Total Fixation Duration [ms]': [],\n",
    "        'Average Fixation Duration [ms]': []\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#prepare_txt_data('data\\\\4\\\\formatted-raw-data\\\\151\\\\', 'ogama.txt', test_fixation_info, call_fixation_detection_on_data)\n",
    "#print(pd.DataFrame(test_fixation_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4590ea7-34fe-48f9-bd73-e7df1eeabbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fixation_data_analysis(directory_path, output_csv = \"pygaze_fixations.csv\"):\n",
    "\n",
    "    # Initialize a dictionary to store the fixation counts, total fixation duration, and average fixation duration for each file\n",
    "    fixation_info = {\n",
    "        'Participant': [],\n",
    "        'Task': [],\n",
    "        'Fixation Count': [],\n",
    "        'Total Fixation Duration [ms]': [],\n",
    "        'Average Fixation Duration [ms]': []\n",
    "    }\n",
    "\n",
    "\n",
    "    # List all files in the directory and subfolders\n",
    "    file_names = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        dir_filenames = [file_name for file_name in files]\n",
    "        file_names[root] = dir_filenames\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for directory, file_names in file_names.items():\n",
    "        for file_name in file_names:\n",
    "            ending = os.path.splitext(file_name)[1]\n",
    "            if ending in ending_to_function:\n",
    "                ending_to_function[ending](directory, file_name, fixation_info, call_fixation_detection_on_data)\n",
    "\n",
    "    # Create a DataFrame to store the fixation information\n",
    "    count_df = pd.DataFrame(fixation_info)\n",
    "\n",
    "    # Define the output CSV file path\n",
    "    output_csv = os.path.join(\"results/\", output_csv)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    count_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the fixation information\n",
    "    print(count_df)\n",
    "\n",
    "    print(f\"Fixation information saved to {output_csv}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0344682c-8131-4fe8-8c4c-0f1f0e92915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def saccade_data_analysis(directory_path, output_csv = \"pygaze_saccades.csv\"):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        directory_path (str): The path to the directory containing the Tobii eye tracker data TSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the saccade information for each file.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the saccade counts, total saccade duration, and average saccade duration for each file\n",
    "    saccade_info = {\n",
    "        'Participant': [],\n",
    "        'Task': [],\n",
    "        'Saccade Count': [],\n",
    "        'Total Saccade Duration [ms]': [],\n",
    "        'Average Saccade Duration [ms]': []\n",
    "    }\n",
    "\n",
    "    # List all files in the directory and subfolders\n",
    "    file_names = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        dir_filenames = [file_name for file_name in files]\n",
    "        file_names[root] = dir_filenames\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for directory, file_names in file_names.items():\n",
    "        for file_name in file_names:\n",
    "            ending = os.path.splitext(file_name)[1]\n",
    "            if ending in ending_to_function:\n",
    "                ending_to_function[ending](directory, file_name, saccade_info, call_saccade_detection_on_data)\n",
    "        \n",
    "            \n",
    "\n",
    "    # Create a DataFrame to store the saccade information\n",
    "    count_df = pd.DataFrame(saccade_info)\n",
    "\n",
    "    output_csv = os.path.join(\"results/\", output_csv)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    count_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the saccade information\n",
    "    print(count_df)\n",
    "\n",
    "    print(f\"Saccade information saved to {output_csv}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397f3e0",
   "metadata": {},
   "source": [
    "The functions are now declared, we can analyze the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de5923d-9d35-4e7d-b874-2357d44ed67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_tobii_data data\\emip_dataset\\rawdata\\100_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\101_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\102_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\103_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\104_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\105_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\106_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\107_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\108_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\109_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\10_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\110_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\111_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\112_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\113_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\114_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\115_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\116_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\117_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\118_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\119_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\11_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\120_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\121_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\122_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\123_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\124_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\125_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\126_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\127_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\128_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\129_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\12_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\130_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\131_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\132_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\133_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\134_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\135_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\136_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\137_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\138_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\139_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\13_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\140_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\141_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\142_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\143_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\144_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\145_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\146_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\147_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\148_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\149_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\14_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\150_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\151_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\152_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\153_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\154_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\155_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\156_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\157_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\158_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\159_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\15_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\160_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\161_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\162_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\163_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\164_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\165_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\166_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\167_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\168_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\169_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\16_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\170_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\171_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\172_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\173_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\174_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\175_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\176_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\177_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\178_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\179_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\17_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\180_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\181_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\182_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\183_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\184_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\185_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\186_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\187_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\188_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\189_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\18_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\190_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\191_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\192_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\193_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\194_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\195_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\196_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\197_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\198_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\199_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\19_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\1_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\200_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\201_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\202_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\203_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\204_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\205_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\206_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\207_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\208_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\209_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\20_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\210_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\211_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\212_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\213_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\214_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\215_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\216_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\21_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\22_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\23_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\24_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\25_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\26_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\27_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\28_rawdata.tsv\n",
      "possible skipped rows worked 45\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\29_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\2_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\30_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\31_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\32_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\33_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\34_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\35_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\36_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\37_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\38_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\39_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\3_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\40_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\41_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\42_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\43_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\44_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\45_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\46_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\47_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\48_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\49_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\4_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\50_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\51_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\52_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\53_rawdata.tsv\n",
      "possible skipped rows worked 41\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\54_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\55_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\56_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\57_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\58_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\59_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\5_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\60_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\61_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\62_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\63_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\64_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\65_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\66_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\67_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\68_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\69_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\6_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\70_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\71_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\72_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\73_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\74_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\75_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\76_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\77_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\78_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\79_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\7_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\80_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\81_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\82_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\83_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\84_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\85_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\86_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\87_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\88_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\89_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\8_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\90_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\91_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\92_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\93_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\94_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\95_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\96_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\97_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\98_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\99_rawdata.tsv\n",
      "possible skipped rows worked 37\n",
      "prepare_tobii_data data\\emip_dataset\\rawdata\\9_rawdata.tsv\n",
      "possible skipped rows worked 32\n",
      "    Participant                           Task  Fixation Count  \\\n",
      "0           100             rectangle_java.jpg              89   \n",
      "1           100  mupliple_choice_rectangle.jpg              78   \n",
      "2           100               vehicle_java.jpg             541   \n",
      "3           100    mupliple_choice_vehicle.jpg              91   \n",
      "4           101            rectangle_java2.jpg             235   \n",
      "..          ...                            ...             ...   \n",
      "859          99    mupliple_choice_vehicle.jpg             195   \n",
      "860           9            rectangle_java2.jpg             191   \n",
      "861           9  mupliple_choice_rectangle.jpg             141   \n",
      "862           9              vehicle_java2.jpg             225   \n",
      "863           9    mupliple_choice_vehicle.jpg             101   \n",
      "\n",
      "     Total Fixation Duration [ms]  Average Fixation Duration [ms]  \n",
      "0                       17182.462                      193.061371  \n",
      "1                       14178.702                      181.778231  \n",
      "2                       98778.660                      182.585323  \n",
      "3                       16578.963                      182.186407  \n",
      "4                       31855.743                      135.556353  \n",
      "..                            ...                             ...  \n",
      "859                     29770.112                      152.667241  \n",
      "860                     29686.256                      155.425424  \n",
      "861                     19753.218                      140.093745  \n",
      "862                     30439.962                      135.288720  \n",
      "863                     14486.232                      143.428040  \n",
      "\n",
      "[864 rows x 5 columns]\n",
      "Fixation information saved to results/pygaze_fixations_emip_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#fixation_data_analysis('data\\\\26\\\\raw_data\\\\', \"pygaze_fixations_26.csv\")\n",
    "#saccade_data_analysis('data\\\\26\\\\raw_data\\\\', \"pygaze_saccades_26.csv\")\n",
    "\n",
    "#fixation_data_analysis('data\\\\54\\\\', \"pygaze_fixations_54.csv\")\n",
    "#saccade_data_analysis('data\\\\54\\\\', \"pygaze_saccades_54.csv\")\n",
    "\n",
    "#fixation_data_analysis('data\\\\4\\\\formatted-raw-data\\\\', \"pygaze_fixations_4_2.csv\")\n",
    "#saccade_data_analysis('data\\\\4\\\\formatted-raw-data\\\\', \"pygaze_saccades_4.csv\")\n",
    "\n",
    "fixation_data_analysis('data\\\\emip_dataset\\\\rawdata\\\\', \"pygaze_fixations_emip_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41015f25",
   "metadata": {},
   "source": [
    "Analyze the orginal data for fixations/saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f159e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "opt = dict()\n",
    "# General variables for eye-tracking data\n",
    "# maximum value of horizontal resolution in pixels\n",
    "opt['xres'] = 1920.0\n",
    "opt['yres'] = 1080.0  # maximum value of vertical resolution in pixels\n",
    "# missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout\n",
    "# internal_helpers as signal for data loss\n",
    "opt['missingx'] = -opt['xres']\n",
    "# missing value for vertical position in eye-tracking data (example data uses -yres). used throughout\n",
    "# internal_helpers as signal for data loss\n",
    "opt['missingy'] = -opt['yres']\n",
    "# sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n",
    "opt['freq'] = 250.0\n",
    "\n",
    "# Variables for the calculation of visual angle\n",
    "# These values are used to calculate noise measures (RMS and BCEA) of\n",
    "# fixations. The may be left as is, but don't use the noise measures then.\n",
    "# If either or both are empty, the noise measures are provided in pixels\n",
    "# instead of degrees.\n",
    "# screen size in cm\n",
    "opt['scrSz'] = [55.0, 32.5]\n",
    "# distance to screen in cm.\n",
    "opt['disttoscreen'] = 65.0\n",
    "\n",
    "# STEFFEN INTERPOLATION\n",
    "# max duration (s) of missing values for interpolation to occur\n",
    "opt['windowtimeInterp'] = 0.1\n",
    "# amount of data (number of samples) at edges needed for interpolation\n",
    "opt['edgeSampInterp'] = 2\n",
    "# maximum displacement during missing for interpolation to be possible\n",
    "opt['maxdisp'] = opt['xres'] * 0.2 * np.sqrt(2)\n",
    "\n",
    "# # K-MEANS CLUSTERING\n",
    "# time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\n",
    "opt['windowtime'] = 0.2\n",
    "# time window shift (s) for each iteration. Use zero for sample by sample processing\n",
    "opt['steptime'] = 0.02\n",
    "# maximum number of errors allowed in k-means clustering procedure before proceeding to next file\n",
    "opt['maxerrors'] = 100\n",
    "opt['downsamples'] = [2, 5, 10]\n",
    "# use chebychev filter when down sampling? 1: yes, 0: no. requires signal processing toolbox. is what matlab's\n",
    "# down sampling internal_helpers do, but could cause trouble (ringing) with the hard edges in eye-movement data\n",
    "opt['downsampFilter'] = False\n",
    "\n",
    "# # FIXATION DETERMINATION\n",
    "# number of standard deviations above mean k-means weights will be used as fixation cutoff\n",
    "opt['cutoffstd'] = 2.0\n",
    "# number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at\n",
    "# fixation ends to refine their placement and stop algorithm from eating into saccades\n",
    "opt['onoffsetThresh'] = 3.0\n",
    "# maximum Euclidean distance in pixels between fixations for merging\n",
    "opt['maxMergeDist'] = 40.0\n",
    "# maximum time in ms between fixations for merging\n",
    "opt['maxMergeTime'] = 60.0\n",
    "# minimum fixation duration after merging, fixations with shorter duration are removed from output\n",
    "opt['minFixDur'] = 90.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe47704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_tobii_data(directory_path, file_name, fixation_info):\n",
    "\ttsv_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\t# Load the Tobii eye tracker data into a Pandas DataFrame\n",
    "\tdf = pd.read_csv(tsv_file, delimiter='\\t', low_memory=False)\n",
    "\tdf = df[['Gaze point X [DACS px]', 'Gaze point Y [DACS px]', 'Recording timestamp [ms]']]\n",
    "\tdf = df.fillna(0.0)\n",
    "\n",
    "\t# Define parameters for fixation detection\n",
    "\tx = df['Gaze point X [DACS px]']  # X-coordinate data\n",
    "\ty = df['Gaze point Y [DACS px]']  # Y-coordinate data\n",
    "\ttime = df['Recording timestamp [ms]']\n",
    "\n",
    "def analyze_csv_data(directory_path, file_name, fixation_info):\n",
    "\tcsv_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\tfile_name_split = file_name.split('-')\n",
    "\ttask_number = file_name_split[0][1:]\n",
    "\tparticipant_id = file_name_split[1]\n",
    "\n",
    "\t# Load the eye tracker data into a Pandas DataFrame\n",
    "\tdf = pd.read_csv(csv_file, delimiter=',', low_memory=False, on_bad_lines='skip')\n",
    "\ttime_column = df.filter(like='TIME(')\n",
    "\n",
    "\ttime_column = time_column.rename(columns={time_column.columns[0]: 'TIME'})\n",
    "\n",
    "\tdf = time_column\n",
    "\n",
    "\t# convert time to milliseconds\n",
    "\tdf['TIME'] = df['TIME']*1000\n",
    "\n",
    "\ttotal_duration = df['TIME'].max() - df['TIME'].min()\n",
    "\taverage_duration = total_duration / len(df['TIME'])\n",
    "\n",
    "\tfixation_info['Participant'].append(participant_id)\n",
    "\tfixation_info['Task'].append(task_number)\n",
    "\tfixation_info['Fixation Count'].append(len(df['TIME']))\n",
    "\tfixation_info['Total Fixation Duration [ms]'].append(total_duration)\n",
    "\tfixation_info['Average Fixation Duration [ms]'].append(average_duration)\n",
    "\n",
    "\n",
    "\n",
    "def analyze_xml_data(directory_path, file_name, fixation_info):\n",
    "\t# We only want the eclipse xml files as those have the x and y coordinates\n",
    "\tif not file_name.contains('eclipse'):\n",
    "\t\treturn\n",
    "\n",
    "\txml_file = os.path.join(directory_path, file_name)\n",
    "\n",
    "\tpath_elements = directory_path.split(os.sep)\n",
    "\tparticipant_id = path_elements[-3]\n",
    "\ttask = path_elements[-2].split('-')[2]\n",
    "\n",
    "\ttree = ET.parse(xml_file)\n",
    "\troot = tree.getroot()\n",
    "\n",
    "\t# Extract data from XML\n",
    "\tdata = []\n",
    "\tfor response_elem in root.findall(\".//response\"):\n",
    "\t\tx = float(response_elem.get(\"x\"))\n",
    "\t\ty = float(response_elem.get(\"y\"))\n",
    "\t\tevent_time = int(response_elem.get(\"event_time\"))\n",
    "\t\t\n",
    "\t\tdata.append({\"event_time\": event_time, \"x\": x, \"y\": y})\n",
    "\n",
    "\t# Create DataFrame\n",
    "\tdf = pd.DataFrame(data)\n",
    "\n",
    "def analyze_csv_data(directory_path, file_name, fixation_info):\n",
    "\tdf_fixation = pd.DataFrame([], columns=[\"Participant\", \"Algorithm\", \"Behavioral\", \"StartTime\", \"EndTime\", \"Duration\", \"IsOutlier\", \"SkillScore\",\n",
    "                                        \"Fixation_startT\", \"Fixation_endT\",  \"Fixation_x\", \"Fixation_y\", \"Fixation_x_range\", \"Fixation_y_range\"])\n",
    "\t#iterate through each row to generate fixation data\n",
    "\tfor index, row in tqdm(df_behavioral.iterrows(), total=len(df_behavioral)):\n",
    "\t\t# read in eyetracking file\n",
    "\t\tdf_eyetracking = pd.read_csv(row[\"Eyetracking\"])\n",
    "\t\t# normalize the time regarding eyetracking to 0\n",
    "\t\tdf_eyetracking[\"time\"] = df_eyetracking[\"time\"].astype(float)\n",
    "\t\tdf_eyetracking[\"time\"] = df_eyetracking[\"time\"] - df_eyetracking[\"time\"].iloc[0]\n",
    "\n",
    "\t\t# drop unused columns\n",
    "\t\tdf_eyetracking = df_eyetracking.drop(columns=[\"l_gaze_point_in_user_coordinate_system_x\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"l_gaze_point_in_user_coordinate_system_y\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"l_gaze_point_in_user_coordinate_system_z\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_point_in_user_coordinate_system_x\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_point_in_user_coordinate_system_y\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_point_in_user_coordinate_system_z\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"l_gaze_origin_in_user_coordinate_system_x\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"l_gaze_origin_in_user_coordinate_system_y\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"l_gaze_origin_in_user_coordinate_system_z\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_origin_in_user_coordinate_system_x\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_origin_in_user_coordinate_system_y\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\"r_gaze_origin_in_user_coordinate_system_z\"])\n",
    "\n",
    "\t\t# convert eyetracking data to display coordinates\n",
    "\t\tdf_eyetracking[\"l_display_x\"] = df_eyetracking[\"l_display_x\"].astype(float) * opt[\"xres\"]\n",
    "\t\tdf_eyetracking[\"l_display_y\"] = df_eyetracking[\"l_display_y\"].astype(float) * opt[\"yres\"]\n",
    "\t\tdf_eyetracking[\"r_display_x\"] = df_eyetracking[\"r_display_x\"].astype(float) * opt[\"xres\"]\n",
    "\t\tdf_eyetracking[\"r_display_y\"] = df_eyetracking[\"r_display_y\"].astype(float) * opt[\"yres\"]\n",
    "\n",
    "\t\t# convert miss column to right integer used by I2MC\n",
    "\t\tdf_eyetracking[\"l_miss_x\"] = df_eyetracking.apply(lambda row: row[\"l_display_x\"] < -opt[\"xres\"] or row[\"l_display_x\"] > 2 * opt[\"xres\"], axis=1)\n",
    "\t\tdf_eyetracking[\"l_miss_y\"] = df_eyetracking.apply(lambda row: row[\"l_display_y\"] < -opt[\"yres\"] or row[\"l_display_y\"] > 2 * opt[\"yres\"], axis=1)\n",
    "\t\tdf_eyetracking[\"r_miss_x\"] = df_eyetracking.apply(lambda row: row[\"r_display_x\"] < -opt[\"xres\"] or row[\"r_display_x\"] > 2 * opt[\"xres\"], axis=1)\n",
    "\t\tdf_eyetracking[\"r_miss_y\"] = df_eyetracking.apply(lambda row: row[\"r_display_y\"] < -opt[\"yres\"] or row[\"r_display_y\"] > 2 * opt[\"yres\"], axis=1)\n",
    "\n",
    "\t\tdf_eyetracking[\"l_miss\"] = df_eyetracking.apply(lambda row: row[\"l_miss_x\"] or row[\"l_miss_y\"] or not row[\"l_valid\"] >= 1, axis=1)\n",
    "\t\tdf_eyetracking[\"r_miss\"] = df_eyetracking.apply(lambda row: row[\"r_miss_x\"] or row[\"r_miss_y\"] or not row[\"r_valid\"] >= 1, axis=1)\n",
    "\n",
    "\t\t# Set a default value for missing data\n",
    "\t\tdf_eyetracking.loc[df_eyetracking[\"l_miss\"], \"l_display_x\"] = opt[\"missingx\"]\n",
    "\t\tdf_eyetracking.loc[df_eyetracking[\"l_miss\"], \"l_display_y\"] = opt[\"missingy\"]\n",
    "\t\tdf_eyetracking.loc[df_eyetracking[\"r_miss\"], \"r_display_x\"] = opt[\"missingx\"]\n",
    "\t\tdf_eyetracking.loc[df_eyetracking[\"r_miss\"], \"r_display_y\"] = opt[\"missingy\"]\n",
    "\n",
    "\t\t# drop unused columns\n",
    "\t\tdf_eyetracking = df_eyetracking.drop(columns=[\"l_miss_x\", \"l_miss_y\", \"r_miss_x\", \"r_miss_y\", \"l_miss\", \"r_miss\"])\n",
    "\n",
    "\t\t# rename columns to match I2MC format\n",
    "\t\tdf_eyetracking.rename(columns={\"l_display_x\": \"L_X\",\n",
    "\t\t\t\t\t\t\t\t\t\"l_display_y\": \"L_Y\",\n",
    "\t\t\t\t\t\t\t\t\t\"r_display_x\": \"R_X\",\n",
    "\t\t\t\t\t\t\t\t\t\"r_display_y\": \"R_Y\",\n",
    "\t\t\t\t\t\t\t\t\t\"l_valid\" : \"LValidity\",\n",
    "\t\t\t\t\t\t\t\t\t\"r_valid\" : \"RValidity\"}, inplace=True)\n",
    "\n",
    "\t\n",
    "original_ending_to_function = {\n",
    "\t'.tsv': analyze_tobii_data,\n",
    "\t'.csv': analyze_csv_data,\n",
    "\t'study4': analyze_csv_study_4_data,\n",
    "\t'.xml': analyze_xml_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26b9df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('data\\\\26\\\\fixations\\\\', ['T0-P108-fixations.csv', 'T0-P164-fixations.csv', 'T0-P165-fixations.csv', 'T0-P169-fixations.csv', 'T0-P193-fixations.csv', 'T0-P228-fixations.csv', 'T0-P309-fixations.csv', 'T0-P313-fixations.csv', 'T0-P320-fixations.csv', 'T0-P322-fixations.csv', 'T0-P327-fixations.csv', 'T0-P340-fixations.csv', 'T0-P370-fixations.csv', 'T0-P372-fixations.csv', 'T0-P376-fixations.csv', 'T0-P379-fixations.csv', 'T0-P382-fixations.csv', 'T0-P393-fixations.csv', 'T0-P431-fixations.csv', 'T0-P435-fixations.csv', 'T0-P442-fixations.csv', 'T0-P459-fixations.csv', 'T0-P490-fixations.csv', 'T0-P523-fixations.csv', 'T0-P539-fixations.csv', 'T0-P548-fixations.csv', 'T0-P561-fixations.csv', 'T0-P604-fixations.csv', 'T0-P606-fixations.csv', 'T0-P620-fixations.csv', 'T0-P623-fixations.csv', 'T0-P627-fixations.csv', 'T0-P637-fixations.csv', 'T0-P641-fixations.csv', 'T0-P709-fixations.csv', 'T0-P718-fixations.csv', 'T0-P722-fixations.csv', 'T0-P742-fixations.csv', 'T0-P745-fixations.csv', 'T0-P763-fixations.csv', 'T0-P783-fixations.csv', 'T0-P785-fixations.csv', 'T0-P797-fixations.csv', 'T0-P821-fixations.csv', 'T0-P825-fixations.csv', 'T0-P831-fixations.csv', 'T0-P855-fixations.csv', 'T0-P867-fixations.csv', 'T0-P876-fixations.csv', 'T0-P901-fixations.csv', 'T0-P906-fixations.csv', 'T0-P920-fixations.csv', 'T0-P932-fixations.csv', 'T0-P952-fixations.csv', 'T0-P963-fixations.csv', 'T1-P108-fixations.csv', 'T1-P164-fixations.csv', 'T1-P165-fixations.csv', 'T1-P169-fixations.csv', 'T1-P193-fixations.csv', 'T1-P228-fixations.csv', 'T1-P309-fixations.csv', 'T1-P313-fixations.csv', 'T1-P320-fixations.csv', 'T1-P322-fixations.csv', 'T1-P327-fixations.csv', 'T1-P340-fixations.csv', 'T1-P370-fixations.csv', 'T1-P372-fixations.csv', 'T1-P376-fixations.csv', 'T1-P379-fixations.csv', 'T1-P382-fixations.csv', 'T1-P393-fixations.csv', 'T1-P431-fixations.csv', 'T1-P435-fixations.csv', 'T1-P442-fixations.csv', 'T1-P459-fixations.csv', 'T1-P490-fixations.csv', 'T1-P523-fixations.csv', 'T1-P539-fixations.csv', 'T1-P548-fixations.csv', 'T1-P561-fixations.csv', 'T1-P604-fixations.csv', 'T1-P606-fixations.csv', 'T1-P620-fixations.csv', 'T1-P623-fixations.csv', 'T1-P627-fixations.csv', 'T1-P637-fixations.csv', 'T1-P641-fixations.csv', 'T1-P709-fixations.csv', 'T1-P718-fixations.csv', 'T1-P722-fixations.csv', 'T1-P742-fixations.csv', 'T1-P745-fixations.csv', 'T1-P763-fixations.csv', 'T1-P783-fixations.csv', 'T1-P785-fixations.csv', 'T1-P797-fixations.csv', 'T1-P821-fixations.csv', 'T1-P825-fixations.csv', 'T1-P831-fixations.csv', 'T1-P855-fixations.csv', 'T1-P867-fixations.csv', 'T1-P876-fixations.csv', 'T1-P901-fixations.csv', 'T1-P906-fixations.csv', 'T1-P920-fixations.csv', 'T1-P932-fixations.csv', 'T1-P952-fixations.csv', 'T1-P963-fixations.csv', 'T10-P108-fixations.csv', 'T10-P164-fixations.csv', 'T10-P165-fixations.csv', 'T10-P169-fixations.csv', 'T10-P193-fixations.csv', 'T10-P228-fixations.csv', 'T10-P309-fixations.csv', 'T10-P313-fixations.csv', 'T10-P320-fixations.csv', 'T10-P322-fixations.csv', 'T10-P327-fixations.csv', 'T10-P340-fixations.csv', 'T10-P370-fixations.csv', 'T10-P372-fixations.csv', 'T10-P376-fixations.csv', 'T10-P379-fixations.csv', 'T10-P382-fixations.csv', 'T10-P393-fixations.csv', 'T10-P431-fixations.csv', 'T10-P435-fixations.csv', 'T10-P442-fixations.csv', 'T10-P459-fixations.csv', 'T10-P490-fixations.csv', 'T10-P523-fixations.csv', 'T10-P539-fixations.csv', 'T10-P548-fixations.csv', 'T10-P561-fixations.csv', 'T10-P604-fixations.csv', 'T10-P606-fixations.csv', 'T10-P620-fixations.csv', 'T10-P623-fixations.csv', 'T10-P627-fixations.csv', 'T10-P637-fixations.csv', 'T10-P641-fixations.csv', 'T10-P709-fixations.csv', 'T10-P718-fixations.csv', 'T10-P722-fixations.csv', 'T10-P742-fixations.csv', 'T10-P745-fixations.csv', 'T10-P763-fixations.csv', 'T10-P783-fixations.csv', 'T10-P785-fixations.csv', 'T10-P797-fixations.csv', 'T10-P821-fixations.csv', 'T10-P825-fixations.csv', 'T10-P831-fixations.csv', 'T10-P855-fixations.csv', 'T10-P867-fixations.csv', 'T10-P876-fixations.csv', 'T10-P901-fixations.csv', 'T10-P906-fixations.csv', 'T10-P920-fixations.csv', 'T10-P932-fixations.csv', 'T10-P952-fixations.csv', 'T10-P963-fixations.csv', 'T2-P108-fixations.csv', 'T2-P164-fixations.csv', 'T2-P165-fixations.csv', 'T2-P169-fixations.csv', 'T2-P193-fixations.csv', 'T2-P228-fixations.csv', 'T2-P309-fixations.csv', 'T2-P313-fixations.csv', 'T2-P320-fixations.csv', 'T2-P322-fixations.csv', 'T2-P327-fixations.csv', 'T2-P340-fixations.csv', 'T2-P370-fixations.csv', 'T2-P372-fixations.csv', 'T2-P376-fixations.csv', 'T2-P379-fixations.csv', 'T2-P382-fixations.csv', 'T2-P393-fixations.csv', 'T2-P431-fixations.csv', 'T2-P435-fixations.csv', 'T2-P442-fixations.csv', 'T2-P459-fixations.csv', 'T2-P490-fixations.csv', 'T2-P523-fixations.csv', 'T2-P539-fixations.csv', 'T2-P548-fixations.csv', 'T2-P561-fixations.csv', 'T2-P604-fixations.csv', 'T2-P606-fixations.csv', 'T2-P620-fixations.csv', 'T2-P623-fixations.csv', 'T2-P627-fixations.csv', 'T2-P637-fixations.csv', 'T2-P641-fixations.csv', 'T2-P709-fixations.csv', 'T2-P718-fixations.csv', 'T2-P722-fixations.csv', 'T2-P742-fixations.csv', 'T2-P745-allgaze.csv', 'T2-P745-fixations.csv', 'T2-P763-fixations.csv', 'T2-P783-fixations.csv', 'T2-P785-fixations.csv', 'T2-P797-fixations.csv', 'T2-P821-fixations.csv', 'T2-P825-fixations.csv', 'T2-P831-fixations.csv', 'T2-P855-fixations.csv', 'T2-P867-fixations.csv', 'T2-P876-fixations.csv', 'T2-P901-fixations.csv', 'T2-P906-fixations.csv', 'T2-P920-fixations.csv', 'T2-P932-fixations.csv', 'T2-P952-fixations.csv', 'T2-P963-fixations.csv', 'T3-P108-fixations.csv', 'T3-P164-fixations.csv', 'T3-P165-fixations.csv', 'T3-P169-fixations.csv', 'T3-P193-fixations.csv', 'T3-P228-fixations.csv', 'T3-P309-fixations.csv', 'T3-P313-fixations.csv', 'T3-P320-fixations.csv', 'T3-P322-fixations.csv', 'T3-P327-fixations.csv', 'T3-P340-fixations.csv', 'T3-P370-fixations.csv', 'T3-P372-fixations.csv', 'T3-P376-fixations.csv', 'T3-P379-fixations.csv', 'T3-P382-fixations.csv', 'T3-P393-fixations.csv', 'T3-P431-fixations.csv', 'T3-P435-fixations.csv', 'T3-P442-fixations.csv', 'T3-P459-fixations.csv', 'T3-P490-fixations.csv', 'T3-P523-fixations.csv', 'T3-P539-fixations.csv', 'T3-P548-fixations.csv', 'T3-P561-fixations.csv', 'T3-P604-fixations.csv', 'T3-P606-fixations.csv', 'T3-P620-fixations.csv', 'T3-P623-fixations.csv', 'T3-P627-fixations.csv', 'T3-P637-fixations.csv', 'T3-P641-fixations.csv', 'T3-P709-fixations.csv', 'T3-P718-fixations.csv', 'T3-P722-fixations.csv', 'T3-P742-fixations.csv', 'T3-P745-fixations.csv', 'T3-P763-fixations.csv', 'T3-P783-fixations.csv', 'T3-P785-fixations.csv', 'T3-P797-fixations.csv', 'T3-P821-fixations.csv', 'T3-P825-fixations.csv', 'T3-P831-fixations.csv', 'T3-P855-fixations.csv', 'T3-P867-fixations.csv', 'T3-P876-fixations.csv', 'T3-P901-fixations.csv', 'T3-P906-fixations.csv', 'T3-P920-fixations.csv', 'T3-P932-fixations.csv', 'T3-P952-fixations.csv', 'T3-P963-fixations.csv', 'T4-P108-fixations.csv', 'T4-P164-fixations.csv', 'T4-P165-fixations.csv', 'T4-P169-fixations.csv', 'T4-P193-fixations.csv', 'T4-P228-fixations.csv', 'T4-P309-fixations.csv', 'T4-P313-fixations.csv', 'T4-P320-fixations.csv', 'T4-P322-fixations.csv', 'T4-P327-fixations.csv', 'T4-P340-fixations.csv', 'T4-P370-fixations.csv', 'T4-P372-fixations.csv', 'T4-P376-fixations.csv', 'T4-P379-fixations.csv', 'T4-P382-fixations.csv', 'T4-P393-fixations.csv', 'T4-P431-fixations.csv', 'T4-P435-fixations.csv', 'T4-P442-fixations.csv', 'T4-P459-fixations.csv', 'T4-P490-fixations.csv', 'T4-P523-fixations.csv', 'T4-P539-fixations.csv', 'T4-P548-fixations.csv', 'T4-P561-fixations.csv', 'T4-P604-fixations.csv', 'T4-P606-fixations.csv', 'T4-P620-fixations.csv', 'T4-P623-fixations.csv', 'T4-P627-fixations.csv', 'T4-P637-fixations.csv', 'T4-P641-fixations.csv', 'T4-P709-fixations.csv', 'T4-P718-fixations.csv', 'T4-P722-fixations.csv', 'T4-P742-fixations.csv', 'T4-P745-fixations.csv', 'T4-P763-fixations.csv', 'T4-P783-fixations.csv', 'T4-P785-fixations.csv', 'T4-P797-fixations.csv', 'T4-P821-fixations.csv', 'T4-P825-fixations.csv', 'T4-P831-fixations.csv', 'T4-P855-fixations.csv', 'T4-P867-fixations.csv', 'T4-P876-fixations.csv', 'T4-P901-fixations.csv', 'T4-P906-fixations.csv', 'T4-P920-fixations.csv', 'T4-P932-fixations.csv', 'T4-P952-fixations.csv', 'T4-P963-fixations.csv', 'T5-P108-fixations.csv', 'T5-P164-fixations.csv', 'T5-P165-fixations.csv', 'T5-P169-fixations.csv', 'T5-P193-fixations.csv', 'T5-P228-fixations.csv', 'T5-P309-fixations.csv', 'T5-P313-fixations.csv', 'T5-P320-fixations.csv', 'T5-P322-fixations.csv', 'T5-P327-fixations.csv', 'T5-P340-fixations.csv', 'T5-P370-fixations.csv', 'T5-P372-fixations.csv', 'T5-P376-fixations.csv', 'T5-P379-fixations.csv', 'T5-P382-fixations.csv', 'T5-P393-fixations.csv', 'T5-P431-fixations.csv', 'T5-P435-fixations.csv', 'T5-P442-fixations.csv', 'T5-P459-fixations.csv', 'T5-P490-fixations.csv', 'T5-P523-fixations.csv', 'T5-P539-fixations.csv', 'T5-P548-fixations.csv', 'T5-P561-fixations.csv', 'T5-P604-fixations.csv', 'T5-P606-fixations.csv', 'T5-P620-fixations.csv', 'T5-P623-fixations.csv', 'T5-P627-fixations.csv', 'T5-P637-fixations.csv', 'T5-P641-fixations.csv', 'T5-P709-fixations.csv', 'T5-P718-fixations.csv', 'T5-P722-fixations.csv', 'T5-P742-fixations.csv', 'T5-P745-fixations.csv', 'T5-P763-fixations.csv', 'T5-P783-fixations.csv', 'T5-P785-fixations.csv', 'T5-P797-fixations.csv', 'T5-P821-fixations.csv', 'T5-P825-fixations.csv', 'T5-P831-fixations.csv', 'T5-P855-fixations.csv', 'T5-P867-fixations.csv', 'T5-P876-fixations.csv', 'T5-P901-fixations.csv', 'T5-P906-fixations.csv', 'T5-P920-fixations.csv', 'T5-P932-fixations.csv', 'T5-P952-fixations.csv', 'T5-P963-fixations.csv', 'T6-P108-fixations.csv', 'T6-P164-fixations.csv', 'T6-P165-fixations.csv', 'T6-P169-fixations.csv', 'T6-P193-fixations.csv', 'T6-P228-fixations.csv', 'T6-P309-fixations.csv', 'T6-P313-fixations.csv', 'T6-P320-fixations.csv', 'T6-P322-fixations.csv', 'T6-P327-fixations.csv', 'T6-P340-fixations.csv', 'T6-P370-fixations.csv', 'T6-P372-fixations.csv', 'T6-P376-fixations.csv', 'T6-P379-fixations.csv', 'T6-P382-fixations.csv', 'T6-P393-fixations.csv', 'T6-P431-fixations.csv', 'T6-P435-fixations.csv', 'T6-P442-fixations.csv', 'T6-P459-fixations.csv', 'T6-P490-fixations.csv', 'T6-P523-fixations.csv', 'T6-P539-fixations.csv', 'T6-P548-fixations.csv', 'T6-P561-fixations.csv', 'T6-P604-fixations.csv', 'T6-P606-fixations.csv', 'T6-P620-fixations.csv', 'T6-P623-fixations.csv', 'T6-P627-fixations.csv', 'T6-P637-fixations.csv', 'T6-P641-fixations.csv', 'T6-P709-fixations.csv', 'T6-P718-fixations.csv', 'T6-P722-fixations.csv', 'T6-P742-fixations.csv', 'T6-P745-fixations.csv', 'T6-P763-fixations.csv', 'T6-P783-fixations.csv', 'T6-P785-fixations.csv', 'T6-P797-fixations.csv', 'T6-P821-fixations.csv', 'T6-P825-fixations.csv', 'T6-P831-fixations.csv', 'T6-P855-fixations.csv', 'T6-P867-fixations.csv', 'T6-P876-fixations.csv', 'T6-P901-fixations.csv', 'T6-P906-fixations.csv', 'T6-P920-fixations.csv', 'T6-P932-fixations.csv', 'T6-P952-fixations.csv', 'T6-P963-fixations.csv', 'T7-P108-fixations.csv', 'T7-P164-fixations.csv', 'T7-P165-fixations.csv', 'T7-P169-fixations.csv', 'T7-P193-fixations.csv', 'T7-P228-fixations.csv', 'T7-P309-fixations.csv', 'T7-P313-fixations.csv', 'T7-P320-fixations.csv', 'T7-P322-fixations.csv', 'T7-P327-fixations.csv', 'T7-P340-fixations.csv', 'T7-P370-fixations.csv', 'T7-P372-fixations.csv', 'T7-P376-fixations.csv', 'T7-P379-fixations.csv', 'T7-P382-fixations.csv', 'T7-P393-fixations.csv', 'T7-P431-fixations.csv', 'T7-P435-fixations.csv', 'T7-P442-fixations.csv', 'T7-P459-fixations.csv', 'T7-P490-fixations.csv', 'T7-P523-fixations.csv', 'T7-P539-fixations.csv', 'T7-P548-fixations.csv', 'T7-P561-fixations.csv', 'T7-P604-fixations.csv', 'T7-P606-fixations.csv', 'T7-P620-fixations.csv', 'T7-P623-fixations.csv', 'T7-P627-fixations.csv', 'T7-P637-fixations.csv', 'T7-P641-fixations.csv', 'T7-P709-fixations.csv', 'T7-P718-fixations.csv', 'T7-P722-fixations.csv', 'T7-P742-fixations.csv', 'T7-P745-fixations.csv', 'T7-P763-fixations.csv', 'T7-P783-fixations.csv', 'T7-P785-fixations.csv', 'T7-P797-fixations.csv', 'T7-P821-fixations.csv', 'T7-P825-fixations.csv', 'T7-P831-fixations.csv', 'T7-P855-fixations.csv', 'T7-P867-fixations.csv', 'T7-P876-fixations.csv', 'T7-P901-fixations.csv', 'T7-P906-fixations.csv', 'T7-P920-fixations.csv', 'T7-P932-fixations.csv', 'T7-P952-fixations.csv', 'T7-P963-fixations.csv', 'T8-P108-fixations.csv', 'T8-P164-fixations.csv', 'T8-P165-fixations.csv', 'T8-P169-fixations.csv', 'T8-P193-fixations.csv', 'T8-P228-fixations.csv', 'T8-P309-fixations.csv', 'T8-P313-fixations.csv', 'T8-P320-fixations.csv', 'T8-P322-fixations.csv', 'T8-P327-fixations.csv', 'T8-P340-fixations.csv', 'T8-P370-fixations.csv', 'T8-P372-fixations.csv', 'T8-P376-fixations.csv', 'T8-P379-fixations.csv', 'T8-P382-fixations.csv', 'T8-P393-fixations.csv', 'T8-P431-fixations.csv', 'T8-P435-fixations.csv', 'T8-P442-fixations.csv', 'T8-P459-fixations.csv', 'T8-P490-fixations.csv', 'T8-P523-fixations.csv', 'T8-P539-fixations.csv', 'T8-P548-fixations.csv', 'T8-P561-fixations.csv', 'T8-P604-fixations.csv', 'T8-P606-fixations.csv', 'T8-P620-fixations.csv', 'T8-P623-fixations.csv', 'T8-P627-fixations.csv', 'T8-P637-fixations.csv', 'T8-P641-fixations.csv', 'T8-P709-fixations.csv', 'T8-P718-fixations.csv', 'T8-P722-fixations.csv', 'T8-P742-fixations.csv', 'T8-P745-fixations.csv', 'T8-P763-fixations.csv', 'T8-P783-fixations.csv', 'T8-P785-fixations.csv', 'T8-P797-fixations.csv', 'T8-P821-fixations.csv', 'T8-P825-fixations.csv', 'T8-P831-fixations.csv', 'T8-P855-fixations.csv', 'T8-P867-fixations.csv', 'T8-P876-fixations.csv', 'T8-P901-fixations.csv', 'T8-P906-fixations.csv', 'T8-P920-fixations.csv', 'T8-P932-fixations.csv', 'T8-P952-fixations.csv', 'T8-P963-fixations.csv', 'T9-P108-fixations.csv', 'T9-P164-fixations.csv', 'T9-P165-fixations.csv', 'T9-P169-fixations.csv', 'T9-P193-fixations.csv', 'T9-P228-fixations.csv', 'T9-P309-fixations.csv', 'T9-P313-fixations.csv', 'T9-P320-fixations.csv', 'T9-P322-fixations.csv', 'T9-P327-fixations.csv', 'T9-P340-fixations.csv', 'T9-P370-fixations.csv', 'T9-P372-fixations.csv', 'T9-P376-fixations.csv', 'T9-P379-fixations.csv', 'T9-P382-fixations.csv', 'T9-P393-fixations.csv', 'T9-P431-fixations.csv', 'T9-P435-fixations.csv', 'T9-P442-fixations.csv', 'T9-P459-fixations.csv', 'T9-P490-fixations.csv', 'T9-P523-fixations.csv', 'T9-P539-fixations.csv', 'T9-P548-fixations.csv', 'T9-P561-fixations.csv', 'T9-P604-fixations.csv', 'T9-P606-fixations.csv', 'T9-P620-fixations.csv', 'T9-P623-fixations.csv', 'T9-P627-fixations.csv', 'T9-P637-fixations.csv', 'T9-P641-fixations.csv', 'T9-P709-fixations.csv', 'T9-P718-fixations.csv', 'T9-P722-fixations.csv', 'T9-P742-fixations.csv', 'T9-P745-fixations.csv', 'T9-P763-fixations.csv', 'T9-P783-fixations.csv', 'T9-P785-fixations.csv', 'T9-P797-fixations.csv', 'T9-P821-fixations.csv', 'T9-P825-fixations.csv', 'T9-P831-fixations.csv', 'T9-P855-fixations.csv', 'T9-P867-fixations.csv', 'T9-P876-fixations.csv', 'T9-P901-fixations.csv', 'T9-P906-fixations.csv', 'T9-P920-fixations.csv', 'T9-P932-fixations.csv', 'T9-P952-fixations.csv', 'T9-P963-fixations.csv'])])\n",
      "    Participant Task  Fixation Count  Total Fixation Duration [ms]  \\\n",
      "0          P108    0              60                      40215.97   \n",
      "1          P164    0              56                      28354.92   \n",
      "2          P165    0              42                      15427.87   \n",
      "3          P169    0              53                      27188.70   \n",
      "4          P193    0             330                     137057.77   \n",
      "..          ...  ...             ...                           ...   \n",
      "601        P906    9              45                      28436.65   \n",
      "602        P920    9             149                      74221.86   \n",
      "603        P932    9             719                     307957.77   \n",
      "604        P952    9             489                     231534.85   \n",
      "605        P963    9             244                      90765.75   \n",
      "\n",
      "     Average Fixation Duration [ms]  \n",
      "0                        670.266167  \n",
      "1                        506.337857  \n",
      "2                        367.330238  \n",
      "3                        512.994340  \n",
      "4                        415.326576  \n",
      "..                              ...  \n",
      "601                      631.925556  \n",
      "602                      498.133289  \n",
      "603                      428.314006  \n",
      "604                      473.486401  \n",
      "605                      371.990779  \n",
      "\n",
      "[606 rows x 5 columns]\n",
      "Fixation information saved to results/pygaze_fixations_original_26.csv\n"
     ]
    }
   ],
   "source": [
    "def original_fixation_data_analysis(directory_path, output_csv = \"pygaze_fixations_original.csv\", fn_type = None):\n",
    "\n",
    "    # Initialize a dictionary to store the fixation counts, total fixation duration, and average fixation duration for each file\n",
    "    fixation_info = {\n",
    "        'Participant': [],\n",
    "        'Task': [],\n",
    "        'Fixation Count': [],\n",
    "        'Total Fixation Duration [ms]': [],\n",
    "        'Average Fixation Duration [ms]': []\n",
    "    }\n",
    "\n",
    "    saccade_info = {\n",
    "        'Participant': [],\n",
    "        'Task': [],\n",
    "        'Saccade Count': [],\n",
    "        'Total Saccade Duration [ms]': [],\n",
    "        'Average Saccade Duration [ms]': []\n",
    "    }\n",
    "\n",
    "\n",
    "    # List all files in the directory and subfolders\n",
    "    file_names = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        dir_filenames = [file_name for file_name in files]\n",
    "        file_names[root] = dir_filenames\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    print(file_names.items())\n",
    "    for directory, file_names in file_names.items():\n",
    "        for file_name in file_names:\n",
    "            ending = os.path.splitext(file_name)[1]\n",
    "            if ending in original_ending_to_function:\n",
    "                if fn_type is not None:\n",
    "                    original_ending_to_function[fn_type](directory, file_name, fixation_info)\n",
    "                else:\n",
    "                    original_ending_to_function[ending](directory, file_name, fixation_info)\n",
    "                \n",
    "\n",
    "    # Create a DataFrame to store the fixation information\n",
    "    count_df = pd.DataFrame(fixation_info)\n",
    "\n",
    "    # Define the output CSV file path\n",
    "    output_csv = os.path.join(\"results/\", output_csv)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    count_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print the fixation information\n",
    "    print(count_df)\n",
    "\n",
    "    print(f\"Fixation information saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b541d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_fixation_data_analysis('data\\\\26\\\\fixations\\\\', \"pygaze_fixations_original_26.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
